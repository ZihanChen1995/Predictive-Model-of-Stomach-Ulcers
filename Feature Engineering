#Import necessary package
import pandas as pd
from numpy import mean
from numpy import std


#Read the two file we have
df1 = pd.read_csv('/chenzihan/lable-1-Data.csv',index_col=0)
df2 = pd.read_csv('/chenzihan/lable-0-Data.csv',index_col=0)

#Drop the coloumn we will not used here
df1 = df1.drop('new_twee_list', axis=1)
df2 = df2.drop('new_twee_list', axis=1)

#Now, combine this two file to make the new data set
df = pd.concat([df1_nor, df2_nor], ignore_index=True)


'''
1 Standard Deviation from the Mean: 68%
2 Standard Deviations from the Mean: 95%
3 Standard Deviations from the Mean: 99.7%
4 Standard Deviations from the Mean: 99.9%
'''

'''
newtweets        1974 non-null int64
newfollowing     1974 non-null int64
newfollower      1974 non-null int64
newlikes         1974 non-null int64
new_join         1974 non-null int64
newpicnum 
'''

#EDA part

#Detect the outlier
print('before')

#Check every column
for col in ['newtweets','newfollowing','newfollower','newlikes','new_join','newpicnum']:
    data = list(df[col])
    # calculate summary statistics
    data_mean, data_std = mean(data), std(data)
    
    # identify outliers
    cut_off = data_std * 4
    lower_line, upper_line = data_mean - cut_off, data_mean + cut_off
    
    # identify outliers
    outliers = [x for x in data if x < lower_line or x > upper_line]
    print(str(col),'Identified outliers: %d' % len(outliers))
    # remove outliers
    outliers_removed = [x for x in data if x >= lower_line and x <= upper_line]
    print(str(col),'Remain: %d' % len(outliers_removed))
    
#Remove the outlier from our dataframe

#Check every column
for col in ['newtweets','newfollowing','newfollower','newlikes','new_join','newpicnum']:
    data = list(df[col])
    # calculate summary statistics
    data_mean, data_std = mean(data), std(data)
    # identify outliers
    cut_off = data_std * 4
    lower_line, upper_line = data_mean - cut_off, data_mean + cut_off
    # Delete outliers
    new_data = []
    for x in data:
        if x < lower_line or x > upper_line:
            out_d = data_mean
        else:
            out_d = x
        new_data.append(out_d)
    #Define new name
    new_na = 'clean_' + col
    df[new_na] = new_data
    df = df.drop(col, axis=1)
    
#Now use Min-Max Normalization
df_nor = (df - df.mean()) / (df.max() - df.min())

#We find label column has been destoried, label again
df_nor['label'] = df_nor['label'].fillna(value=1)
df_nor['label'] = df_nor['label'].astype(int) 

#Now check which kinds of features should we throw into our final model

#Random Forest Prediction
import numpy as np
from sklearn.cross_validation import cross_val_score, ShuffleSplit
from sklearn.ensemble import RandomForestRegressor


#format dataframe without responds
factor=df[['clean_newtweets','clean_newfollowing','clean_newfollower','clean_newlikes','clean_new_join','clean_newpicnum']]
X =np.array(factor)
Y =np.array(df['label'])
names = np.array(factor.columns)

rf = RandomForestRegressor(n_estimators=10, max_depth=None)

#Use a list to contain the scores we want to check
scores = []

for i in range(X.shape[1]):
    score = cross_val_score(rf, X[:, i:i+1], Y, scoring="r2", 
                            cv=ShuffleSplit(len(X), 3, .3))
    scores.append((format(np.mean(score), '.3f'), names[i]))
    
#Now it's time to check which feature should we choose
print(sorted(scores, reverse=True))

df.to_csv('final_stage_data.csv')
    
        
