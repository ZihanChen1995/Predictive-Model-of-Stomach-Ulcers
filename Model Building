#In this part, I firstly choose 10 kinds of mainstream machine learning methods, then choose the Top 3 based on their performance.
#In the last parts, I will use the GridSearch to tune the hyper-parameters and find the best one for our model.

#The first one is the model based on Numerical Data.

#Import necessary packages
import pandas as pd
import warnings

#Avoid unnecessary warning
warnings.filterwarnings("ignore")

#read the file
df = pd.read_csv('/chenzihan/final_stage_data.csv',index_col=0)

variables = ['clean_newtweets','clean_newfollowing','clean_newfollower','clean_newlikes','clean_new_join','clean_newpicnum']
X = df[variables]
y = df['label']

from sklearn.model_selection import cross_val_score

#We use cross-validation here firstly to choose the best model
#Then we use gridsearch to find the best hyper parameters
#Introduce the algorithm pool
#1.Logistic Regression
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()

#2.KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 3)

#3.Naive Bayes
from sklearn.naive_bayes import GaussianNB
gaussian = GaussianNB()

#4.Perceptron
from sklearn.linear_model import Perceptron
perceptron = Perceptron()

#5.Neural Network
from sklearn.neural_network import MLPClassifier
mlpc = MLPClassifier()

#6.SVC
from sklearn.svm import LinearSVC
linear_svc = LinearSVC()

#7.SGDClassifier
from sklearn import linear_model
sgd = linear_model.SGDClassifier()

#8.Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
decision_tree = DecisionTreeClassifier()

#9.Random Forest
from sklearn.ensemble import RandomForestClassifier
random_forest = RandomForestClassifier()

#10.AdaBoost
from sklearn.ensemble import AdaBoostClassifier
ada = AdaBoostClassifier()

#Set a function to run this list
def run_model(function_name, X, y, cv_number):
    
    scores = cross_val_score(function_name,X,y,cv=cv_number,scoring='accuracy')
    
    #Return Result
    result = scores.mean()
    
    return result

#List all the method we mentioned before in a list
mach_list = [logreg,knn,gaussian,perceptron,mlpc,linear_svc,sgd,decision_tree,random_forest,ada]

final_result = []
for al in mach_list:
    #Use folds 10
    final_result.append(run_model(al,X, y, 10))

print(final_result)



#Now we turn into the model of textual data
#import package
import pandas as pd
import string
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import PorterStemmer
from nltk.tokenize import WordPunctTokenizer
from nltk.tokenize import word_tokenize

#Read the two file we have, with unique encode
df1 = pd.read_csv('/chenzihan/text-Data-normal.csv',index_col=0,encoding = 'ISO-8859-1')
df2 = pd.read_csv('/chenzihan/text-Data-ulcer.csv',index_col=0,encoding = 'ISO-8859-1') 

#Now, combine this two file to make the new data set
df = pd.concat([df1, df2], ignore_index=True)

#Define label
label = list(df['label'])


#Define a function to clean this dataXszcx 
def data_clean(input_data):
    
    #Firstly, split into word
    input_data = str(input_data)
    in_data_1 = word_tokenize(input_data)
    
    #Then lower the case
    in_data_2 = [word.lower() for word in in_data_1]
    
    #Then move away the punctation
    trans = str.maketrans('', '', string.punctuation)
    in_data_3 = [word.translate(trans) for word in in_data_2]
    
    #Now we have remove all the punctation
    #Next step, remove not alphabetic
    in_data_4 = [word for word in in_data_3 if word.isalpha()]
    
    #Then filter out stop words
    stop_words = set(stopwords.words('english'))
    in_data_5 = [word for word in in_data_4 if not word in stop_words]
    
    #Final Step, reducing each word to its root or base
    porter = PorterStemmer()
    new_data = [porter.stem(word) for word in in_data_5]
    
    #return result
    return new_data

new_twee_list_1 = list(df['new_twee_list'])

#Use the second function to clean the new_twee_list again
new_twee_list_2 = []

#Run the loop again
for ele in new_twee_list_1:
    #Just use the function we used before to clean the data
    mid_con = data_clean(ele)
    #Append to new list
    new_twee_list_2.append(mid_con)
    
#Use the second function to clean the new_twee_list again
new_twee_list_3 = []
    
#Here is the thing, we need to make the list into string
for ele in new_twee_list_2:
    
    #Define intial word
    int_word = 'a,'
    for ele_2 in ele:
        #Make dic to string
        int_word = int_word + ele_2 + ' '
    #Append to new list
    new_twee_list_3.append(int_word[2:])

#Put it back to original dataframe
df['clean_twee_list'] = new_twee_list_3

#Drop the original row      
df = df.drop('new_twee_list', axis=1)

#Transfer it into TFIDF
tfidf_vect = TfidfVectorizer(stop_words="english", min_df=5)
text = list(df['clean_twee_list'])
dtm = tfidf_vect.fit_transform(text)

# use MultinomialNB algorithm
from sklearn.naive_bayes import MultinomialNB

# import method for split train/test data set
from sklearn.model_selection import train_test_split

#Import necessary package to measure the accurcy
from sklearn.metrics import accuracy_score


result = []
for run in range(0,100):

    # split dataset into train (70%) and test sets (30%)
    X_train, X_test, y_train, y_test = train_test_split(dtm, label, test_size=0.3) 
    #random_state means the split data will not change if it is run for more than one time, get same answer
    
    # train a multinomial naive Bayes model using the testing data
    clf = MultinomialNB().fit(X_train, y_train)
    
    # predict the news group for the test dataset
    predicted=clf.predict(X_test)
    
    score = accuracy_score(y_test,predicted)
    
    result.append(score)

scores = sum(result)/len(result)

print(scores)
print(result)
